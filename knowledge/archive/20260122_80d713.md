---
title: RAGの精度が73%から100%に向上した話 ─ チャンキング戦略の比較検証
url: https://zenn.dev/oharu121/articles/efd3d038afc6da
captured_at: 2026-01-22
published_at: 2026-01-22
---

# RAGの精度が73%から100%に向上した話 ─ チャンキング戦略の比較検証

## TL;DR & Key Conclusions
社内規程文書を対象としたRAGシステムで、チャンキング戦略の改善により回答精度を73.3%から100%に向上させた検証報告。意外にも、Large Chunk（2000文字）というシンプルな手法が最も効果的で、Re-rankingは逆に精度を悪化させた。RAG精度改善の鍵は、LLMの最適化ではなくデータ品質と検索精度にある。

- **Large Chunkが最強**: 2000文字のシンプルなチャンキングで100%の精度を達成。複雑なParent-ChildやHypothetical Questionsより効果的だった
- **Re-rankingの落とし穴**: Cross-Encoderによる再ランキングを導入したところ、精度が73.3%から60.0%に悪化。Recall（再現率）の問題にはPrecision向上ツールは無力
- **検索失敗の本質はRecall不足**: RAGシステムの精度が上がらない原因は、LLMではなく検索（Retrieval）の問題。初回検索で必要な情報を取得できなければ、Re-rankingでも救えない
- **例外規定の埋没問題**: 日本の社内規程特有の構造（一般規則→特例→附則）により、ベクトル検索で例外規定が埋没しやすい
- **暗黙的参照の課題**: 文書内の「第2条の2に定める者」とユーザーの「アルバイト」のエイリアス問題により、ベクトル検索が失敗する
- **マルチホップ推論の困難**: 複数の情報源が離れた場所にある場合、比較クエリ（「正社員とアルバイトの差額は？」）に回答できない
- **データ品質が最も重要**: チャンキング戦略の改善には限界があり、文書の前処理（従業員タイプ別の分割など）が根本的な解決策になりうる
- **ヘッダーベースチャンキングの可能性**: 固定長ではなくMarkdownの見出しで分割することで、意味的なまとまりを保てる
- **クエリ拡張の有効性**: ユーザークエリを文書内の用語に変換してから検索することで、エイリアス問題を緩和できる
- **システム構成**: Next.js（フロントエンド）+ FastAPI（バックエンド）+ multilingual-e5-large（埋め込み）+ Chroma DB（ベクトル検索）+ Gemini 2.0 Flash（LLM）
- **評価方法の確立**: 15問の評価クエリに必須キーワードと禁止キーワードを設定し、自動評価を実現
- **検証の再現性**: 意図的に失敗するデータセットを設計することで、チャンキング戦略の効果を定量的に検証

## Quickstart
RAGシステムのチャンキング戦略を改善し、精度を向上させるための手順：

1. **現状分析**: 自分のデータセットで、どのような検索失敗が起きているかを分析する。評価用クエリを作成し、必須キーワードと禁止キーワードを設定して自動評価できるようにする
2. **ベースライン確立**: まずは標準的なチャンキング（1000文字、オーバーラップ200文字）で精度を測定し、ベースラインを確立する
3. **Large Chunk試行**: 最もシンプルな改善として、チャンクサイズを2000文字に増やし、オーバーラップを500文字に設定して精度を測定する
4. **問題パターンの特定**: 暗黙的参照、マルチホップ推論、例外規定の埋没、否定クエリのどれが主要な失敗原因かを特定する
5. **戦略の選択**: Parent-Child Chunking（検索とコンテキストを分離）、Hypothetical Questions（想定質問の事前生成）、ヘッダーベースチャンキングなど、問題に応じた戦略を選択する
6. **Re-ranking慎重導入**: Re-rankingはRecall問題には効果がない。Precision向上が必要な場合のみ導入し、必ず検証する
7. **データ品質改善**: チャンキング戦略で限界を感じたら、文書の前処理（従業員タイプ別の分割、クエリ拡張、検索件数の増加）を検討する
8. **継続的検証**: セマンティックチャンキング、GraphRAG、ファインチューニング済み埋め込みモデルなど、新しい手法の検証を継続する

## Context & Claims (Claim-Evidence-Caveat)

- **Claim**: Large Chunk（2000文字）が最も効果的で100%の精度を達成した
    - **Evidence**: Standard Chunking（1000文字）は73.3%、Large Chunking（2000文字）は100%、Parent-Child Chunkingは93.3%、Hypothetical Questionsは93.3%、Re-rankingは60.0%という検証結果
    - **Caveat**: この結果はデータセット依存。例外規定が一般規則から300〜500文字以内に配置されていたため、2000文字で両方を含むことができた。例外規定が1000文字以上離れている場合は失敗する可能性がある。チャンクが大きすぎると、検索精度が下がるリスクもある

- **Claim**: Re-rankingは万能ではなく、逆に精度を悪化させることがある
    - **Evidence**: Re-ranking導入により、精度が73.3%から60.0%に低下した実測結果。Re-rankingはPrecision（精度）を上げるツールであり、Recall（再現率）の問題には効果がない
    - **Caveat**: 今回の問題の本質が検索漏れ（Recall不足）だったため、Re-rankingが効果を発揮できなかった。初回検索で十分な候補が取得でき、ノイズ除去が必要な場合はRe-rankingが有効な可能性もある

- **Claim**: RAG精度改善の鍵は、LLMではなくデータ品質と検索精度にある
    - **Evidence**: RAGシステムの精度が上がらない原因の多くは検索（Retrieval）の問題。暗黙的参照、マルチホップ推論、例外規定の埋没、否定クエリなど、検索失敗のパターンが特定された
    - **Caveat**: この主張は社内規程文書という特定のドメインでの検証結果。他のドメイン（技術文書、FAQ、ニュース記事など）では異なる課題が存在する可能性がある

- **Claim**: 日本の社内規程特有の構造が、RAGシステムの精度を下げる要因となる
    - **Evidence**: 一般規則（全従業員向け、キーワード密度高）→特例（特定従業員向け、キーワード密度低）→附則（例外ケース、さらにキーワード密度低）という構造により、ベクトル検索で例外規定が埋没する
    - **Caveat**: この構造は日本企業の規程特有のものであり、欧米企業や他の文書タイプには当てはまらない可能性がある

- **Claim**: エイリアス問題（同一概念の複数呼称）が検索精度を下げる
    - **Evidence**: 文書内の「第2条の2に定める者」「短期雇用者」とユーザーの「アルバイト」、文書内の「パートタイム従業員」「週所定労働日数4日以上の者」とユーザーの「パート」など、用語の不一致により検索が失敗
    - **Caveat**: クエリ拡張やHypothetical Questionsで緩和できるが、LLMのAPI呼び出しコストが増加する。文書の前処理で用語を統一する方が根本的な解決策になりうる

- **Claim**: Parent-Child Chunkingは網羅的な回答に有効だが、比較クエリでは失敗する
    - **Evidence**: 93.3%の精度を達成したが、「正社員とアルバイトの差額」のような比較クエリで1問失敗。2つの金額が異なる親チャンクに分かれていたため
    - **Caveat**: 親チャンクのサイズを調整するか、複数の親チャンクを同時に取得する仕組みを導入すれば改善の余地がある

- **Claim**: Hypothetical Questionsはエイリアス問題の解決に効果的
    - **Evidence**: 93.3%の精度を達成。各チャンクに対してLLMで想定質問を事前生成し、「第2条の2に定める者」から「アルバイトの通勤手当の上限はいくらですか？」を生成することで検索精度が向上
    - **Caveat**: インデックス作成時にLLMのAPI呼び出しが発生し、コストが増加する。チャンク数が多い場合は現実的でない可能性がある

- **Claim**: データ品質の改善が最も根本的な解決策
    - **Evidence**: 従業員タイプごとに文書を分割し、それぞれの視点から書き直すことで93.3%の精度を達成。チャンキング戦略の改善には限界があることが判明
    - **Caveat**: 文書の前処理には人的コストがかかり、文書が更新されるたびにメンテナンスが必要。運用コストとのトレードオフを考慮する必要がある

- **Claim**: ヘッダーベースチャンキングで意味的なまとまりを保てる
    - **Evidence**: Markdownの見出し（##、###）で分割することで、固定長チャンキングより意味的に一貫したチャンクを作成できる可能性がある
    - **Caveat**: まだ実装・検証されていない改善案。見出しの粒度が適切でない場合、チャンクサイズがばらつき、検索精度が不安定になる可能性がある

- **Claim**: クエリ拡張でエイリアス問題を緩和できる
    - **Evidence**: ユーザーの「アルバイトの通勤手当は？」を「アルバイト 短期雇用者 第2条の2に定める者 通勤手当 交通費」に拡張することで、文書内の用語と一致させられる
    - **Caveat**: クエリごとにLLMのAPI呼び出しが発生し、レスポンスタイムとコストが増加する。リアルタイム性が求められる用途では不向き

- **Claim**: 検索件数（k）の増加でRecallを向上できる
    - **Evidence**: 取得件数kを4から10に増やすことで、例外規定が含まれる確率を上げられる
    - **Caveat**: kを増やしすぎるとコンテキストが長くなり、LLMの処理コストが増加する。また、ノイズが増えて回答品質が低下するリスクもある

## Code Snippets
記事には以下のコードスニペットが含まれています：

- **Standard Chunking設定**: 最も基本的なチャンキング戦略（chunk_size=1000、overlap=200）のパラメータ設定
- **Large Chunking設定**: シンプルだが効果的なチャンキング戦略（chunk_size=2000、overlap=500）のパラメータ設定
- **Parent-Child Chunking設定**: 検索とコンテキストを分離する2階層チャンキング（child_chunk_size=400、parent_chunk_size=2000）のパラメータ設定
- **Hypothetical Questions生成ロジック**: 各チャンクに対してLLMで想定質問を事前生成し、ベクトル化してインデックスするコード
- **Re-ranking設定**: Cross-Encoderによる再ランキング（initial_k=10、final_k=4、model="cross-encoder/ms-marco-MiniLM-L-6-v2"）のパラメータ設定
- **システムアーキテクチャ図**: Next.js（フロントエンド）→ FastAPI（バックエンド）→ multilingual-e5-large（埋め込み）+ Chroma DB（ベクトル検索）+ Gemini 2.0 Flash（LLM）の構成図
- **通勤手当規程の例**: 一般規則（第3条で上限50,000円）と例外規定（第12条で第2条の2に定める者は20,000円）が離れて配置されている実例
- **評価用クエリJSON**: 必須キーワード（["20,000円", "2万円"]）と禁止キーワード（["50,000円"]）を設定した自動評価用クエリ
- **ヘッダーベースチャンキングの概念**: Markdownの見出しで分割する方法の擬似コード
- **クエリ拡張の処理フロー**: ユーザークエリをLLMで文書内の用語に変換する処理の例
- **検索件数増加の設定**: kを4から10に増やすパラメータ変更例
- **データ品質改善の構造**: 統合された規程文書を従業員タイプ別（正社員、パート、アルバイト）に分割する前処理の例

## Normalized Conditions

- **Use Cases**:
    - 社内規程文書や業務マニュアルなど、例外規定が多く含まれる文書のRAGシステム構築
    - 日本企業特有の文書構造（一般規則→特例→附則）を持つドキュメントの検索精度向上
    - エイリアス問題（同一概念の複数呼称）が顕著な文書のRAGシステム
    - マルチホップ推論や比較クエリが頻繁に発生するドメイン
    - チャンキング戦略の効果を定量的に検証したい開発者
    - RAGシステムの精度が70〜80%台で頭打ちになっている場合
    - Re-rankingを導入したが期待した効果が得られなかった場合の原因分析
    - ベクトル検索でRecall（再現率）の問題に直面しているプロジェクト

- **Anti-Cases**:
    - 文書構造がシンプルで、例外規定がほとんど存在しない場合
    - エイリアス問題が少なく、用語が統一されている文書
    - 単一の情報源で回答できる単純なFAQシステム
    - リアルタイム性が最優先で、精度向上のためのコスト増加を許容できない場合
    - データセットが小規模で、全文検索で十分な精度が得られる場合
    - 文書が頻繁に更新され、データ品質改善のメンテナンスコストが見合わない場合
    - 技術文書やニュース記事など、日本企業の規程文書とは異なる構造の文書
    - すでに100%に近い精度が出ている場合（改善の余地が少ない）

- **Prerequisites**:
    - RAGシステムの基本的な構築スキル（Next.js、FastAPI、Chroma DB、LangChainなど）
    - ベクトル検索と埋め込みモデルの基本理解
    - 評価用クエリの作成と自動評価の仕組みの構築能力
    - チャンキング戦略の効果を検証するための実験環境
    - LLM APIへのアクセス（Gemini、OpenAI、Claudeなど）
    - 対象ドメインの文書構造と典型的なクエリパターンの理解
    - PrecisionとRecallの概念理解
    - Cross-Encoderなど再ランキング手法の基礎知識（Re-ranking検証時）

- **Decision Triggers**:
    - RAGシステムの精度が70%台で頭打ちになり、改善策を模索している
    - チャンクサイズを変更することで精度が向上する可能性を確認したい
    - Re-rankingを導入したが逆に精度が悪化し、原因を分析したい
    - 例外規定や特例規定の検索漏れが頻発している
    - ユーザーのクエリと文書内の用語が一致せず、検索精度が低い
    - マルチホップ推論や比較クエリで失敗が多発している
    - チャンキング戦略の効果を定量的に検証し、最適な手法を選択したい
    - データ品質改善（文書の前処理）とチャンキング戦略のどちらを優先すべきか判断したい
    - Recall不足とPrecision不足のどちらが主要な問題かを特定したい
