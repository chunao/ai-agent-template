---
title: AI に指示するのではなく、まずは AI に質問させた方が捗る
url: https://zenn.dev/hataluck/articles/d347a5011cf71e
captured_at: 2026-01-21
published_at: 2026-01-21
---

# AI に指示するのではなく、まずは AI に質問させた方が捗る

## TL;DR & Key Conclusions
暗黙知を言語化するには、AIに指示を出すより「AIに質問させる」インタビュー型プロンプティングが効果的です。LLMは入力に含まれない情報を一般論で補完するため、不足情報を質問させることで暗黙知を引き出し、AIの推論精度と自己理解の両方を向上させることができます。

- **インタビュー型プロンプティングの本質**: AIに答えさせるのではなく質問させることで、言語化できていない前提・制約・判断基準をあぶり出す
- **LLMの特性の活用**: 不足情報を一般論で補完する性質を逆手に取り、質問を通じて推論材料を揃える
- **メタ認知の促進**: AIの質問によって自分の思考を一段上から見る視点が得られる
- **3つの型の要素**: 役割の固定、質問の質と量の制約、終了条件の設定がインタビューを成功に導く
- **技術設計での活用**: トレードオフ表作成において、暗黙の判断基準を明示化して議論をスムーズにする
- **幅広い適用範囲**: 技術設計だけでなく、記事構成作成やキャリア振り返りなど言語化が難しい場面全般に有効
- **質問の質の確保**: 表面的・自明な質問を避け、最大3問に制限することで思考のラリーを継続可能にする
- **適切な終了**: ユーザーまたはAIが十分と判断した時点でアウトプットに移行し、質問の無限ループを防ぐ
- **チーム議論の促進**: 判断軸と前提を見える化することで、「なんとなく」ではない構造的な議論が可能になる

## Quickstart
1. **役割とゴールを明確化**: AIに「技術設計のレビュワー」「編集者」「コーチ」など具体的な役割を設定し、暗黙知を引き出すというゴールを指示
2. **質問の制約を設定**: 表面的・自明な質問を避けるよう指示し、最大3問ずつに制限して質問の質を高める
3. **終了条件を明示**: ユーザーが「十分」と言うか、AIが十分と判断したらアウトプット（トレードオフ表、構成案など）を作成するよう指定
4. **初期メモを提供**: 現時点で言語化できている情報（要件、想定、制約など）を箇条書きで入力
5. **対話を継続**: AIの質問に答えながら暗黙知を言語化し、十分な情報が揃ったら最終アウトプットを依頼

## Context & Claims (Claim-Evidence-Caveat)
- **Claim**: LLMに指示するより質問させる方が暗黙知の言語化に効果的である
    - **Evidence**: LLMは入力に含まれない情報を学習データの一般論で補完するため、不足情報を質問させることで推論材料を揃えられる。技術設計での実例では、「取りこぼし最小化の定義」「通知の種類」「遅延の上限」など暗黙の判断基準を引き出せた
    - **Caveat**: 質問を制約しないと大量の表面的な質問が返され、思考のラリーが継続できなくなる

- **Claim**: インタビュー型プロンプティングはメタ認知を促す
    - **Evidence**: AIの質問によって自分が言語化できていなかった前提・判断基準・制約に気づき、自分の思考を一段上から見ることができる
    - **Caveat**: 効果を得るには質問の質が重要で、表面的な質問では深い気づきは得られない

- **Claim**: 3つの要素（役割固定、質問制約、終了条件）がインタビュー型プロンプティングの成功に不可欠
    - **Evidence**: 役割の固定で質問の方向性が定まり、質問制約（最大3問、自明な質問禁止）で質が保たれ、終了条件で適切なタイミングでアウトプットに移行できる
    - **Caveat**: 終了条件がないと質問が延々続き、実用的なアウトプットが得られない

- **Claim**: トレードオフ表作成において判断軸と前提が見える化される
    - **Evidence**: 通知配信の技術選定例では、同期処理/非同期キュー/バッチ処理の比較において各方式の評価軸が明確になった
    - **Caveat**: 記事では仮の評価例のみで、実際の数値やデータによる裏付けは示されていない

- **Claim**: 技術設計以外でも言語化が難しい場面全般に適用可能
    - **Evidence**: 記事構成作成（編集者型）やキャリア振り返り（コーチング型）のプロンプト例が示されている
    - **Caveat**: 各用途での実際の効果は具体例が限定的で、詳細な検証は不足

## Code Snippets
該当なし

## Normalized Conditions
- **Use Cases**:
    - 技術設計やアーキテクチャ選定でトレードオフ表を作成したいとき
    - 技術記事を書く際に読者にとって価値のある構成を考えたいとき
    - キャリアや目標の振り返りで自己理解を深めたいとき
    - チーム内で暗黙知を共有し、議論の前提を揃えたいとき
    - 複雑な意思決定において判断基準を明確化したいとき
- **Anti-Cases**:
    - 単純な事実確認や情報検索を求める場合
    - 既に明確な要件定義が済んでいて、実装のみが必要な場合
    - 質問に答える時間がなく、即座にアウトプットが必要な場合
    - 対話形式での思考整理を望まない場合
- **Prerequisites**:
    - LLMサービス（ChatGPT、Claude等）へのアクセス
    - 暗黙知を言語化したい具体的なテーマまたは課題
    - AIとの対話を継続する時間的余裕
    - プロンプトテンプレートの基本理解
- **Decision Triggers**:
    - 技術選定で「なんとなく」ではなく構造的な判断をしたいとき
    - 記事や文書を書く際に「何を伝えたいか」が曖昧なとき
    - チーム内で前提のすれ違いが生じているとき
    - 自分の思考を整理し、メタ認知を得たいとき
- **Failure Modes / Risks**:
    - 質問の質と量を制約しないと、大量の表面的な質問が返される
    - 終了条件を設けないと質問が延々と続き、実用的なアウトプットに到達できない
    - AIに的確な役割を設定しないと、質問の方向性がぶれる
    - 初期メモが不十分だとAIが質問する材料がなく、一般論しか返ってこない
- **Operational Cost**:
    - 対話を継続するための時間コスト（複数ラウンドの質問と回答）
    - プロンプトテンプレートの作成と調整のコスト
    - AIサービスのAPI利用料（対話回数に応じて増加）
    - 得られたアウトプットをチームで共有・議論するコスト
