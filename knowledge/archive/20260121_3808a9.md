---
title: 同じ社内ツールをOpenAI・Claude（MCP）・Geminiから呼ぶ最小設計
url: https://note.com/tasty_dunlin998/n/n08fe02bb92cf
captured_at: 2026-01-21
published_at: 2026-01-21
---

# 同じ社内ツールをOpenAI・Claude（MCP）・Geminiから呼ぶ最小設計

## TL;DR & Key Conclusions
複数のLLMプラットフォーム（OpenAI、Claude、Gemini）で同じツールを呼べるようにするための最小設計を解説。ツール契約の共通化と実行ハーネスの標準化により、プロバイダ依存の差分を薄いアダプタ層に閉じ込めることで、スキルのポータビリティと保守性を確保する。

- **共通スキーマの定義**: JSON Schemaで入出力を固定し、requiredとadditionalProperties: falseを必ず設定することでモデルの暴走を防ぐ
- **実行ハーネスの標準化**: 入力検証、認可、冪等性、リトライ、構造化ログ、縮退処理を全環境で統一することで品質を保証
- **プロバイダ差分の分離**: OpenAI（tools）、Claude（MCP）、Gemini（function calling）の差分を薄いアダプタ層に押し込み、スキル本体は変更不要にする
- **命名の固定化**: ツール名（例：db_search）と引数名（query、top_kなど）を環境間で不変に保つことで、プロンプトと評価の一貫性を維持
- **証拠ベースの返却**: 検索結果にはid、title、score、snippetを含め、モデルには「この証拠だけで要約せよ」と指示することで検証可能性を確保
- **段階的導入**: 既存の社内APIに1環境でツール呼び出しを実装し、構造化ログで失敗を回収してから2環境目以降をアダプタ追加で展開
- **スキーマ厳格化**: additionalProperties: falseを基本とし、曖昧なobjectや自由形式の辞書を避けることでモデルが暴れるリスクを低減
- **結果サイズの制御**: 全件返却を避け、証拠として必要な粒度にスニペットを整形することでコスト爆発を防ぐ
- **失敗時の可視化**: 「静かに壊れる」を防ぐため、縮退経路を決定し、失敗を必ず構造化ログに残す
- **セキュリティ**: 最小権限でツールトークンを発行し、読み取り系と書き込み系でスコープを分離、ログのマスキングを実施
- **再現性の担保**: request_idを全レイヤで引き回し、スキーマ検証を必須化、失敗時の縮退ルールを事前定義
- **評価軸の設定**: 関連性、網羅性、説明性、応答時間、コストの5軸で、正解がない世界でもツール品質を評価可能にする

## Quickstart
1. **共通スキーマの作成**: 社内ツール（例：db_search）の入出力をJSON Schemaで定義し、requiredとadditionalProperties: falseを設定
2. **1環境での実装**: OpenAI、Claude、Geminiのいずれか1つでtool callingの往復を実装し、構造化ログを追加
3. **ハーネスの追加**: 入力検証、認可、冪等性、リトライ、縮退処理を実装し、request_idを全レイヤで引き回す
4. **2環境目以降の展開**: 共通スキーマを各プロバイダ形式に変換する薄いアダプタを追加し、スキル本体は変更せず展開

## Context & Claims (Claim-Evidence-Caveat)
- **Claim**: 複数LLMプラットフォームでツール連携がバラバラになると、保守が崩壊する
    - **Evidence**: モデルやプラットフォームが増えるほど、同じ機能（検索、要約、参照）を各環境で作り直す必要があり、認可、ログ、評価、失敗時の挙動が場当たりになる
    - **Caveat**: 本質的な問題はモデルの性能ではなく、ツールのI/Oと実行ハーネスが移植可能な形になっていないこと

- **Claim**: ツール契約を共通化し、実行ハーネスを標準化すれば、プロバイダ差分はアダプタに閉じ込められる
    - **Evidence**: 中層（スキーマ、命名、戻り値）と下層（認可、検証、ログ、冪等、リトライ、縮退）を統一することで、上層（プロンプト、計画、評価）の差分のみがモデル依存になる
    - **Caveat**: プロバイダごとのSDKやレスポンス形式は更新されるため、実装時は公式リファレンスに合わせる必要がある

- **Claim**: JSON SchemaでrequiredとadditionalProperties: falseを設定することで、モデルの暴走を防げる
    - **Evidence**: スキーマがゆるいと、関係ないフィールドが混ざりやすく、曖昧なobjectや自由形式の辞書ではモデルが暴れやすい
    - **Caveat**: スキーマは最小で固め、必要なときにのみ拡張する運用が必要

- **Claim**: MCPは複数の社内ツールに接続する際に相性が良い
    - **Evidence**: MCPサーバがツール群を公開し、Claude側（クライアント）が必要なツールだけを呼ぶ設計により、ツール定義と実装が分離され、移植性が向上する
    - **Caveat**: MCPの実装はHTTPでサーバを叩く形式であり、Claude Code以外でも利用可能だが、プロバイダ差分はMCPクライアント層で吸収する必要がある

- **Claim**: 検索結果を証拠として扱うことで、検証可能性を確保できる
    - **Evidence**: id、title、score、snippetを返し、モデルには「この証拠だけで要約せよ」と指示することで、出典を辿れるようにする
    - **Caveat**: 全件返却を避け、証拠として必要な粒度にスニペットを整形しないと、コストが爆発する

- **Claim**: 失敗時に「静かに壊れる」ことが本番で最も困る
    - **Evidence**: 縮退経路を決定し、失敗を必ず構造化ログに残すことで、後で調査できる形を保証する
    - **Caveat**: 最低限、tool、request_id、latency_ms、status、error_type、tenantを構造化ログで残す必要がある

- **Claim**: 小さく始めることで、最初からマルチ環境対応を避けられる
    - **Evidence**: 既存の社内検索APIに入出力のJSON Schemaをかぶせ、1環境でtool callingの往復を通し、構造化ログで失敗を回収してから、2環境目以降をアダプタ追加で展開する
    - **Caveat**: 段階的導入により、初期の複雑さを回避できるが、最終的には共通スキーマと実行ハーネスの標準化が必要

## Code Snippets
- **入力スキーマ（db_search.request.json）**: 必須フィールドとadditionalProperties: falseを含むJSON Schema定義
- **出力スキーマ（db_search.response.json）**: hits配列とその要素（id、title、score、snippet）を定義するJSON Schema
- **OpenAI tool calling実装**: tools配列を渡し、tool callを受け取り、スキーマ検証後にdb_search APIを実行し、結果を返す
- **Claude MCP実装**: MCPサーバにHTTP POSTでdb_searchツールを実行させ、結果を取得する
- **Gemini function calling実装**: 関数宣言を渡し、関数呼び出しを受け取り、スキーマ検証後にdb_search APIを実行し、結果を返す

## Normalized Conditions
- **Use Cases**:
    - 社内DBや外部APIへの検索・要約・参照機能を複数のLLMプラットフォームで統一的に提供したい場合
    - OpenAI、Claude、Geminiなど複数のモデルを同時に運用し、ツールの移植性を確保したい場合
    - チーム開発でツール実装が各環境でバラバラになり、保守コストが増大している場合
    - 認可、ログ、評価、失敗時の挙動を標準化し、品質を保証したい場合
    - MCPを活用してツール定義と実装を分離し、移植性を高めたい場合
- **Anti-Cases**:
    - 単一のLLMプラットフォームのみで運用し、他環境への展開予定がない場合
    - プロトタイピングや単発タスクで、ツールの移植性や標準化が不要な場合
    - ツール呼び出しの頻度が低く、個別実装のコストが許容できる場合
- **Prerequisites**:
    - OpenAI、Claude、GeminiのいずれかのAPIアクセス権限
    - JSON Schemaの基本知識
    - ツール実行基盤（社内DB検索API、MCP環境など）
    - 構造化ログの仕組み（ツール名、request_id、レイテンシ、ステータス、エラータイプを記録）
    - 認可とトークン管理の仕組み
- **Decision Triggers**:
    - 複数のLLMプラットフォームを並行運用する必要が生じたとき
    - 各環境でツール実装が重複し、保守コストが増大してきたとき
    - 認可、ログ、評価、失敗時の挙動が場当たりになり、品質が不安定になったとき
    - スキルのポータビリティを確保し、モデル選定の自由度を高めたいとき
- **Failure Modes / Risks**:
    - スキーマがゆるく、モデルが暴れて予期しないフィールドが混入する
    - ツール結果が大きすぎて、コストが爆発する
    - 失敗時に「静かに壊れる」ことで、本番で原因不明の障害が発生する
    - プロバイダごとのSDKやレスポンス形式の更新に追従できず、実装が陳腐化する
    - 初期のマルチ環境対応で複雑さが増し、導入コストが高くなる
- **Operational Cost**:
    - 初期コスト：共通スキーマ定義、1環境でのtool calling実装、構造化ログ追加、実行ハーネス（検証・認可・冪等・リトライ・縮退）の実装
    - 拡張コスト：2環境目以降のアダプタ追加（プロバイダ差分の吸収層）
    - 維持コスト：スキーマのバージョン管理、各プロバイダSDKの更新追従、構造化ログの監視と失敗パターンの回収、縮退ルールの見直し
    - 学習コスト：JSON Schema、OpenAI/Claude/Gemini各APIのtool calling仕様、MCP（Claude）の理解
